{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a model trained on a historical demand dataset, that can forecast demand on a Hold-out test dataset. The model should be able to accurately forecast ahead by T+1 to T+5 time intervals (where each interval is 15-min) given all data up to time T.\n",
    "\n",
    "step by step documentation on how to run your code. Our evaluators will be running your data models on a test dataset.\n",
    "\n",
    "The given dataset contains normalised historical demand of a city, aggregated spatiotemporally within geohashes and over 15 minute intervals. The dataset spans over a two month period.\n",
    "\n",
    "\n",
    "- geohash6: geohash level 6. Geohash is a public domain geocoding system which encodes a geographic location into a short string of letters and digits with arbitrary precision. You are free to use any geohash library to encode/decode the geohashes into latitude and longitude or vice versa. Some examples include https://github.com/hkwi/python-geohash (for Python).\n",
    "- day: day, where the value indicates the sequential order and not a particular day of the month\n",
    "- timestamp: start time of 15-minute intervals, in the following format: hour:minute, where hour ranges from 0 to 23 and minute is either one of (0, 15, 30, 45)\n",
    "- demand: aggregated demand normalised to be in the range [0,1]\n",
    "\n",
    "\n",
    "Test dataset details:\n",
    "\n",
    "1. Timeframe: The test dataset can start from any time period after the timeframe of the training dataset. Your model can use features of up to 14 consecutive days from the test dataset, ending at timestamp T and predict T+1 to T+5.\n",
    "\n",
    "\n",
    "2. Geohash coverage: You may assume that the set of geohashes are the same in training dataset and test dataset. The original geohashes are anonymised (it may not be on an existing city), but you may assume that adjacency is maintained between the geohashes.\n",
    "\n",
    "\n",
    "Submissions will be evaluated by RMSE (root mean squared error) averaged over all geohash6, 15-minute-bucket pairs.\n",
    "\n",
    "[Data Source](https://s3-ap-southeast-1.amazonaws.com/grab-aiforsea-dataset/traffic-management.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # NOTE: there are 9 missing timesteps:\n",
    "    #       1671, 1672, 1673, 1678, 1679, 1680, 1681, 1682, 1683\n",
    "    \n",
    "Enforce training on test 14d\n",
    "\n",
    "batch prediction\n",
    "zone function, 7*7zone code mapping dictionary\n",
    "spatial lag layer\n",
    "Surge: indicator? Neighbors?\n",
    "examine zeros\n",
    "Why KNN is a bad idea\n",
    "Compare with AAAI paper\n",
    "- with seasonal diff: rolling mean and std\n",
    "\n",
    "- those that remain constant throughout the week\n",
    "- those that has different am/pm demands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# local import\n",
    "from rdforecast import datasets\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "path = './input/'\n",
    "output_size = 5  # timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data\n",
    "- Either through local module functions or manually load it into the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'filepath' not given, download data from: https://s3-ap-southeast-1.amazonaws.com/grab-aiforsea-dataset/traffic-management.zip\n",
      "Data loaded.\n",
      "N: 4206321\n",
      "  geohash6  day timestamp    demand\n",
      "0   qp03wc   18      20:0  0.020072\n",
      "1   qp03pn   10     14:30  0.024721\n",
      "2   qp09sw    9      6:15  0.102821\n"
     ]
    }
   ],
   "source": [
    "# retrieve data from source url (if filepath is None)\n",
    "data = datasets.load_training_data(filepath=None)\n",
    "data = datasets.check_sanity(data)\n",
    "train, test = datasets.split_train_test(data)  # specify path to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'filepath' not given, download data from: https://s3-ap-southeast-1.amazonaws.com/grab-aiforsea-dataset/traffic-management.zip\n",
      "Data loaded.\n",
      "N: 4206321\n",
      "  geohash6  day timestamp    demand\n",
      "0   qp03wc   18      20:0  0.020072\n",
      "1   qp03pn   10     14:30  0.024721\n",
      "2   qp09sw    9      6:15  0.102821\n"
     ]
    }
   ],
   "source": [
    "data = datasets.load_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 47 days\n",
      "Testing set: 14 days\n"
     ]
    }
   ],
   "source": [
    "test_days = 14\n",
    "total_days = data['day'].max()\n",
    "cutpoint = total_days - test_days\n",
    "train = data[data['day'] <= cutpoint]\n",
    "test = data[data['day'] > cutpoint]\n",
    "print('Training set: {} days'.format(len(train['day'].unique())))\n",
    "print('Testing set: {} days'.format(len(test['day'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "Scaling by Max-Min: This is good and often required preprocessing for Linear models, Neural Networks\n",
    "\n",
    "Normalization using Standard Deviation: This is good and often required preprocessing for Linear models, Neural Networks\n",
    "\n",
    "Log-based feature/Target: Use log based features or log-based target function. If one is using a Linear model which assumes that the features are normally distributed, a log transformation could make the feature normal. It is also handy in case of skewed variables like income.\n",
    "\n",
    "\n",
    "\n",
    "Time segmentation\n",
    "Manual relate temporal features\n",
    "Manually tag sth, domain specific\n",
    "\n",
    "normalize your inputs so that the average is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection and Sampling and Splitting\n",
    "- spatial sampling?\n",
    "- imbalance issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Training\n",
    "- Training score\n",
    "- Validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation (Backtesting)\n",
    "- RMSE\n",
    "- MAPE\n",
    "- visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "assert len(y_true) == len(y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "# rmse = np.sqrt(np.average((y_true - y_pred) ** 2))\n",
    "mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Selection\n",
    "- ANN stacking with models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Output\n",
    "- result log for validation\n",
    "- test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
